
val lines = sc.textFile("movie/imdb_labelled.txt")

val columns = lines.map{_.split("\\t")}

import spark.sqlContext.implicits._
case class Review(text: String, label: Double)
val reviews = columns.map{a => Review(a(0),a(1).toDouble)}.toDF()

val Array(trainingData, testData) = reviews.randomSplit(Array(0.8, 0.2))

-- instantiate and tokenize the input "text" to new column "words"
import org.apache.spark.ml.feature.Tokenizer
val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words")
val tokenizedData = tokenizer.transform(trainingData)

-- HashingTF is a Transformer that converts a sequence of words into a fixed-length feature Vector. It maps
-- a sequence of terms to their term frequencies using a hashing function
import org.apache.spark.ml.feature.HashingTF
val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol).setOutputCol("features")

-- train a LogisticRegression model over the training data
import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01)

-- assemble a machine learning pipeline.
import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))

-- train a model using pipeline.
val pipeLineModel = pipeline.fit(trainingData)

-- evaluate how the generated model performs on both the training and test dataset
val trainingPredictions = pipeLineModel.transform(trainingData)

val testPredictions = pipeLineModel.transform(testData)

-- We will use a binary classifier evaluator to evaluate our model. It expects two input columns: rawPrediction and label.
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val evaluator = new BinaryClassificationEvaluator()

-- evaluate our model using Area under ROC as a metric. 
import org.apache.spark.ml.param.ParamMap
val evaluatorParamMap = ParamMap(evaluator.metricName -> "areaUnderROC")

val aucTest = evaluator.evaluate(testPredictions, evaluatorParamMap)
aucTest: Double = 0.6827956989247317